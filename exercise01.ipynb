{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning: Architectures and Methods (Summer 2019) - Exercise 1\n",
    "<div style=\"text-align:right\">Stefan Thaut<br>\n",
    "Department 20 - Computer Science<br>\n",
    "Technische Universit√§t Darmstadt<br><br><br>\n",
    "</div>\n",
    "In this exercise a feed-forward neural network will be implemented.\n",
    "<br>\n",
    "<center>\n",
    "<div style=\"display:inline-block;text-align:center\">\n",
    "<img src=\"nn_architecture.png\" width=\"300px\"/>\n",
    "<span style=\"display:block\">Figure 1: The architecture of the network implemented in this exercise</span>\n",
    "</div>\n",
    "</center>\n",
    "<br>\n",
    "We follow an object-oriented approach in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "We have to import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the Model\n",
    "We define the sigmoid function, that can be applied on scalars as well as on vectors or even on matrices, as follows:\n",
    "\\begin{align}\n",
    "\\text{sigmoid}(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{align}\n",
    "The neural network is modelled as a class. The constructor expects the input of the network, the number of hidden layers and the output corresponding to the given input. The network can be initialized with predefined weights and biases, otherwise it will create random weights and will not use biases.<br><br>\n",
    "\n",
    "Predefined weights and biases have to be passed as lists to the constructor in form of keyword arguments with the keys \"weights\" and \"biases\".<br>\n",
    "Assuming ```W_1``` and ```W_2``` are weight matrices and ```b_1``` and ```b_2``` are biases, an exemplary call of the constructor would be\n",
    "```python\n",
    "NeuralNetwork(x, 1, y, weights = [W_1, W_2], biases = [b_1, b_2])```\n",
    "If the network is not initialized with predefined weights and biases, the number of neurons per hidden layer optionally can be defined.<br>\n",
    "In the current model each hidden layer consists of the same number of neurons and each hidden layer uses the sigmoid function as activation function.<br><br>\n",
    "\n",
    "The neural network class provides a ```feed_forward()```-method, that performs one run of the calculations from the input through each hidden layer to the output. It prints the output matrix of each layer and also returns a list of this matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, x, num_hidden, y, **kwargs):\n",
    "        self.input = x\n",
    "        self.num_hidden = num_hidden\n",
    "        if 'weights' in kwargs and isinstance(kwargs['weights'], list) and len(kwargs['weights']) == num_hidden + 1:\n",
    "            self.weights = kwargs['weights']\n",
    "        else:\n",
    "            self.weights = []\n",
    "            if 'num_neurons' in kwargs:\n",
    "                self.num_neurons = kwargs['num_neurons']\n",
    "            else:\n",
    "                self.num_neurons = 10\n",
    "            \n",
    "            if num_hidden == 1:\n",
    "                self.weights.append(np.random.rand(x.shape[1], y.shape))\n",
    "            else:\n",
    "                self.weights.append(np.random.rand(x.shape[1], self.num_neurons))\n",
    "                \n",
    "                for i in range(num_hidden - 2):\n",
    "                    self.weights.append(np.random.rand(self.weights[-1].shape[1], self.num_neurons))\n",
    "                \n",
    "                self.weights.append(np.random.rand(self.weights[-1].shape[1], y.shape[0]))\n",
    "        \n",
    "        if 'biases' in kwargs and isinstance(kwargs['biases'], list) and len(kwargs['biases']) == num_hidden + 1:\n",
    "            self.biases = []\n",
    "            for bias in kwargs['biases']:\n",
    "                if bias.ndim == 1:\n",
    "                    self.biases.append(bias.reshape(-1, 1))\n",
    "                else:\n",
    "                    self.biases.append(bias)\n",
    "        else:\n",
    "            self.biases = []\n",
    "            for i in range(num_hidden + 1):\n",
    "                self.biases.append(np.zeros(self.weights[i].shape[0]).reshape(-1, 1))\n",
    "    \n",
    "    def feed_forward(self):\n",
    "        output = []\n",
    "        temp_value = sigmoid(np.add(np.matmul(self.weights[0], self.input), self.biases[0]))\n",
    "        output.append(temp_value)\n",
    "        print('Output of hidden layer 1')\n",
    "        print(temp_value)\n",
    "        print()\n",
    "        \n",
    "        for i in range(self.num_hidden - 1):\n",
    "            temp_value = sigmoid(np.add(np.matmul(self.weights[i + 1], temp_value), self.biases[i + 1]))\n",
    "            output.append(temp_value)\n",
    "            print('Output of hidden layer', i + 2)\n",
    "            print(temp_value)\n",
    "            print()\n",
    "        \n",
    "        temp_value = np.add(np.matmul(self.weights[-1], temp_value), self.biases[-1])\n",
    "        output.append(temp_value)\n",
    "        print('Output of the output layer')\n",
    "        print(temp_value)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the Input, Weights and Biases\n",
    "The weights and the input are (```numpy```-)matrices and the biases and the output are (one-dimensional) vectors. To be mathematical correct, the biases actually must be column-vectors, but the model checks and converts that by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1 = np.array([[-0.2, 0.48, -0.52],\n",
    "               [-0.56, 1.97, 1.39],\n",
    "               [0.1, 0.28, 0.77],\n",
    "               [1.25, 1.01, -1.3]])\n",
    "\n",
    "b_1 = np.array([0.27, 0.23, 1.35, 0.89])\n",
    "\n",
    "W_2 = np.array([[-1, -0.19, 0.83, -0.22],\n",
    "                [-0.27, 0.24, 1.62, -0.51],\n",
    "                [-0.29, 0.06, 0.15, 0.26],\n",
    "                [0, 0.67, -0.36, -0.42]])\n",
    "\n",
    "b_2 = np.array([-1.19, -0.93, -0.43, 0.28])\n",
    "\n",
    "W_3 = np.array([[-0.13, 0.01, -0.1, 0.03],\n",
    "                [-0.24, -0.02, -0.15, -0.1]])\n",
    "\n",
    "b_3 = np.array([-0.13, 0.03])\n",
    "\n",
    "X = np.array([[0.13, 0.68, 0.8, 0.57, 0.97],\n",
    "              [0.63, 0.89, 0.5, 0.35, 0.71],\n",
    "              [0.5, 0.23, 0.24, 0.79, 0.5]])\n",
    "\n",
    "y = np.zeros(2).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed the Network forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(X, 2, y, weights = [W_1, W_2, W_3], biases = [b_1, b_2, b_3])\n",
    "outputs = nn.feed_forward()\n",
    "\n",
    "print()\n",
    "print()\n",
    "print('Outputs transposed for filling the table')\n",
    "print()\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.transpose())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "After executing the above code, we get the following results for the given weights, biases and the input:\n",
    "\n",
    "|$x_1$|$x_2$|$x_3$|$h_1^{(1)}$|$h_2^{(1)}$|$h_3^{(1)}$|$h_4^{(1)}$|$h_1^{(2)}$|$h_2^{(2)}$|$h_3^{(2)}$|$h_4^{(2)}$|$o_1$|$o_2$\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---\n",
    "|$0.13$|$0.63$|$0.50$|$0.57$|$0.89$|$0.87$|$0.74$|$0.20$|$0.54$|$0.45$|$0.56$|$-0.18$|$-0.15$\n",
    "|$0.68$|$0.89$|$0.23$|$0.61$|$0.87$|$0.86$|$0.91$|$0.19$|$0.51$|$0.45$|$0.54$|$-0.18$|$-0.15$\n",
    "|$0.80$|$0.50$|$0.24$|$0.56$|$0.75$|$0.85$|$0.89$|$0.20$|$0.51$|$0.45$|$0.53$|$-0.18$|$-0.15$\n",
    "|$0.57$|$0.35$|$0.79$|$0.48$|$0.85$|$0.89$|$0.72$|$0.22$|$0.56$|$0.45$|$0.56$|$-0.18$|$-0.16$\n",
    "|$0.97$|$0.71$|$0.50$|$0.54$|$0.86$|$0.88$|$0.90$|$0.21$|$0.53$|$0.46$|$0.54$|$-0.18$|$-0.15$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
